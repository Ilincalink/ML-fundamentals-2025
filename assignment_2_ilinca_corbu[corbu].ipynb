{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/Ilincalink/ML-fundamentals-2025.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"hour.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the target variable\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['cnt'], kde=True, bins=30, color='blue')\n",
    "plt.title('Distribution of variable (cnt)')\n",
    "plt.xlabel('cnt')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Check skewness of the target variable\n",
    "print(f'Skewness of cnt: {df[\"cnt\"].skew()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from this first graph we can see that the distribution is very skewed to the right, meaning that the mode is to the furthest left, followed by the median and then the mean. The skeweness value of 1.2774 further proves that this is a positive skew, with most values being to the left and the aforementioned positions of the median mean and mode. The right hand-side also shows there might be some outliers I have to take into consideration, that could otherwise distrupt the future system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cnt by hour with the updated method\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='hr', y='cnt', data=df, hue='hr', legend=False)\n",
    "plt.title('cnt by Hour')\n",
    "plt.xlabel('Hour of the Day')\n",
    "plt.ylabel('cnt')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the count changes depending on the hour, with the peaks at 8 o'clock and 17 o'clock both with approximately 600. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cnt by weekday with the updated method\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='weekday', y='cnt', data=df, hue='weekday', legend=False)\n",
    "plt.title('cnt by Weekday')\n",
    "plt.xlabel('Weekday')\n",
    "plt.ylabel('cnt')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weekday plot shows a more distributed pattern, showing that the count is balanced throughout the week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cnt by month with the updated method\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='mnth', y='cnt', data=df, hue='mnth', legend=False)\n",
    "plt.title('cnt by Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('cnt')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows how the usage of bicycles grows in the hot months and gradually lowers as the weather gets colder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cnt by season with the updated method\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='season', y='cnt', data=df, hue='season', legend=False)\n",
    "plt.title('cnt by Season')\n",
    "plt.xlabel('Season')\n",
    "plt.ylabel('cnt')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As fully expected, the highest usage of the bikes is in the summer season, followed by spring and autumn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cnt by holiday with the updated method\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='holiday', y='cnt', data=df, hue='holiday', legend=False)\n",
    "plt.title('cnt by Holiday')\n",
    "plt.xlabel('Holiday')\n",
    "plt.ylabel('cnt')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cnt by workingday with the updated method\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='workingday', y='cnt', data=df, hue='workingday', legend=False)\n",
    "plt.title('cnt by Working Day')\n",
    "plt.xlabel('Working Day')\n",
    "plt.ylabel('cnt')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cnt by temperature (temp)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='temp', y='cnt', data=df, color='green')\n",
    "plt.title('cnt by Temperature')\n",
    "plt.xlabel('Temperature')\n",
    "plt.ylabel('cnt')\n",
    "plt.show()\n",
    "\n",
    "# Plot cnt by apparent temperature (atemp)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='atemp', y='cnt', data=df, color='red')\n",
    "plt.title('cnt by Apparent Temperature (atemp)')\n",
    "plt.xlabel('Apparent Temperature')\n",
    "plt.ylabel('cnt')\n",
    "plt.show()\n",
    "\n",
    "# Plot cnt by humidity (hum)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='hum', y='cnt', data=df, color='blue')\n",
    "plt.title('cnt by Humidity')\n",
    "plt.xlabel('Humidity')\n",
    "plt.ylabel('cnt')\n",
    "plt.show()\n",
    "\n",
    "# Plot cnt by windspeed\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='windspeed', y='cnt', data=df, color='purple')\n",
    "plt.title('cnt by Windspeed')\n",
    "plt.xlabel('Windspeed')\n",
    "plt.ylabel('cnt')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have decided to gather the plots regarding humidity, temperature and windspeed together as I believe having them one after another can give more insight into how such weather events truly effect our 'cnt' variable. From the plots above we can see a strong correlation between weather and the cnt of bikes. The higher the temperature, the higher the usage of bikes, whereas higher wind speeds lower the number of people using this method of transport. I found the humidity plot to be the most interesting one, as we can see an almost normal distribution between 'cnt' and this change in weather. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cnt by weather situation (weathersit)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='weathersit', y='cnt', data=df, hue='weathersit', legend=False)\n",
    "plt.title('cnt by Weather Situation')\n",
    "plt.xlabel('Weather Situation')\n",
    "plt.ylabel('cnt')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for outliers using boxplots\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='cnt', data=df, color='orange')\n",
    "plt.title('Outliers in cnt')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the irrelevant columns\n",
    "df = df.drop(columns=['instant', 'dteday', 'casual', 'registered'])\n",
    "\n",
    "# Verify the new structure of the dataframe\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df.corr()\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, \n",
    "            annot=True, \n",
    "            cmap='coolwarm', \n",
    "            center=0,\n",
    "            fmt='.2f',\n",
    "            square=True)\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this correlation table we can see that atemp and temp are very highly correlated and that this could also be the case for holiday and workingday even though it is not as drastic. I will look into this in the future to make sure no other variables are 'leaky' or redundant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=['yr', 'mnth', 'hr'])\n",
    "\n",
    "# Calculate the number of rows in the dataset\n",
    "n_rows = len(df)\n",
    "\n",
    "# Calculate split indices\n",
    "train_size = int(0.6 * n_rows)\n",
    "validation_size = int(0.2 * n_rows)\n",
    "\n",
    "# Split the dataset\n",
    "train_data = df[:train_size]\n",
    "validation_data = df[train_size:train_size + validation_size]\n",
    "test_data = df[train_size + validation_size:]\n",
    "\n",
    "train_data['cnt'] = train_data['cnt'].astype(float)\n",
    "validation_data['cnt'] = validation_data['cnt'].astype(float)\n",
    "test_data['cnt'] = test_data['cnt'].astype(float)\n",
    "\n",
    "# Now apply log transformation\n",
    "train_data['cnt'] = np.log1p(train_data['cnt'])\n",
    "validation_data['cnt'] = np.log1p(validation_data['cnt'])\n",
    "test_data['cnt'] = np.log1p(test_data['cnt'])\n",
    "\n",
    "\n",
    "# Print the size of each split\n",
    "print(f\"Training set size: {train_data.shape[0]}\")\n",
    "print(f\"Validation set size: {validation_data.shape[0]}\")\n",
    "print(f\"Test set size: {test_data.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the total number of rows in the dataset\n",
    "total_rows = len(df)\n",
    "print(f\"Total number of rows in the dataset: {total_rows}\")\n",
    "\n",
    "# Verify the sum of the splits\n",
    "assert (train_data.shape[0] + validation_data.shape[0] + test_data.shape[0]) == total_rows, \"The split sizes don't add up!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure my data in temporaly correct and put in order, I ordered it by month, year, hour, to make sure that the training set was before the validation which came before the test set. I did this to ensure that my groups would not contain data leakage therefore giving me better results. Finally, in the second part of my code I checked the total number of rows in my dataset to make sure that it is the sum that i get from adding the test, training and validation sets in order to make sure all my data is used efficiently and effectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Encode cyclical features (hr, weekday)\n",
    "def encode_cyclical(df, column, max_value):\n",
    "    df[column + '_sin'] = np.sin(2 * np.pi * df[column] / max_value)\n",
    "    df[column + '_cos'] = np.cos(2 * np.pi * df[column] / max_value)\n",
    "    return df\n",
    "\n",
    "# Apply cyclical encoding to 'hr' (hour of the day) and 'weekday'\n",
    "df = encode_cyclical(df, 'hr', 24)  # 24 hours in a day\n",
    "df = encode_cyclical(df, 'weekday', 7)  # 7 days in a week\n",
    "\n",
    "# Step 2: One-hot encoding for categorical features (season, weathersit, mnth)\n",
    "# We can apply this after splitting to avoid data leakage\n",
    "categorical_columns = ['season', 'weathersit', 'mnth']\n",
    "\n",
    "# Step 3: Apply scaling (StandardScaler) to continuous features (temp, hum, windspeed)\n",
    "# Remove 'atemp' from the continuous features list as it's dropped later in the code\n",
    "continuous_columns = ['temp', 'hum', 'windspeed', 'temp_hum_interaction']\n",
    "\n",
    "# Step 4: Interaction terms (temp * hum)\n",
    "df['temp_hum_interaction'] = df['temp'] * df['hum']\n",
    "\n",
    "# Step 5: Remove leaky or redundant features (e.g., drop 'atemp' if highly correlated with 'temp')\n",
    "df.drop(columns=['atemp'], inplace=True)  # Dropping 'atemp' column\n",
    "\n",
    "# Splitting the data into training, validation, and test sets\n",
    "# Assuming you have already split the dataset as per the previous task\n",
    "train_data = df[:10427]\n",
    "validation_data = df[10427:10427+3475]\n",
    "test_data = df[10427+3475:]\n",
    "\n",
    "# Separate target variable 'cnt' from features\n",
    "X_train = train_data.drop(columns=['cnt'])\n",
    "y_train = train_data['cnt']\n",
    "X_val = validation_data.drop(columns=['cnt'])\n",
    "y_val = validation_data['cnt']\n",
    "X_test = test_data.drop(columns=['cnt'])\n",
    "y_test = test_data['cnt']\n",
    "\n",
    "# Column Transformer: Apply one-hot encoding and scaling\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_columns),  # One-hot encode categorical columns\n",
    "        ('num', StandardScaler(), continuous_columns),  # Scale continuous features and interaction terms\n",
    "        ('cyc', 'passthrough', ['hr_sin', 'hr_cos', 'weekday_sin', 'weekday_cos'])  # Keep cyclical features as they are\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a pipeline for preprocessing\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "# Fit the preprocessor on the training data and apply to the validation and test sets\n",
    "X_train_transformed = pipeline.fit_transform(X_train)\n",
    "X_val_transformed = pipeline.transform(X_val)\n",
    "X_test_transformed = pipeline.transform(X_test)\n",
    "\n",
    "# Verify the transformed shapes\n",
    "print(f\"Transformed X_train shape: {X_train_transformed.shape}\")\n",
    "print(f\"Transformed X_val shape: {X_val_transformed.shape}\")\n",
    "print(f\"Transformed X_test shape: {X_test_transformed.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred = model.predict(X_val_transformed)\n",
    "\n",
    "# Calculate performance metrics\n",
    "mse = mean_squared_error(y_val, y_val_pred)\n",
    "mae = mean_absolute_error(y_val, y_val_pred)\n",
    "r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"R² Score: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals\n",
    "residuals = y_val - y_val_pred\n",
    "\n",
    "# Plotting residuals\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(residuals, kde=True)\n",
    "plt.title('Residuals Distribution')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Plotting residuals vs predicted values\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(y_val_pred, residuals)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.title('Residuals vs Predicted')\n",
    "plt.xlabel('Predicted values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Initialize Random Forest Regressor with default parameters\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model on the transformed training data\n",
    "rf_model.fit(X_train_transformed, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on validation set\n",
    "y_val_rf_pred = rf_model.predict(X_val_transformed)\n",
    "\n",
    "# Evaluate performance\n",
    "rf_mse = mean_squared_error(y_val, y_val_rf_pred)\n",
    "rf_mae = mean_absolute_error(y_val, y_val_rf_pred)\n",
    "rf_r2 = r2_score(y_val, y_val_rf_pred)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"Random Forest Regressor Performance:\")\n",
    "print(f\"Mean Squared Error (MSE): {rf_mse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {rf_mae}\")\n",
    "print(f\"R² Score: {rf_r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Comparison with Linear Regression ---\")\n",
    "print(f\"Linear Regression R²: {r2:.4f}\")\n",
    "print(f\"Random Forest R²: {rf_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf_model.feature_importances_\n",
    "\n",
    "# Get feature names from the pipeline\n",
    "# You have one-hot encoded features, so we’ll extract all feature names\n",
    "ohe = pipeline.named_steps['preprocessor'].named_transformers_['cat']\n",
    "ohe_feature_names = ohe.get_feature_names_out(categorical_columns)\n",
    "all_feature_names = list(ohe_feature_names) + continuous_columns + ['hr_sin', 'hr_cos', 'weekday_sin', 'weekday_cos']\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'Feature': all_feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(data=feature_importances_df.head(15), x='Importance', y='Feature')\n",
    "plt.title('Top 15 Feature Importances - Random Forest')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "lgb_model = LGBMRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "lgb_model.fit(X_train_transformed, y_train)\n",
    "y_val_lgb_pred = lgb_model.predict(X_val_transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the validation set\n",
    "lgb_mse = mean_squared_error(y_val, y_val_lgb_pred)\n",
    "lgb_mae = mean_absolute_error(y_val, y_val_lgb_pred)\n",
    "lgb_r2 = r2_score(y_val, y_val_lgb_pred)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"LightGBM Regressor Performance:\")\n",
    "print(f\"Mean Squared Error (MSE): {lgb_mse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {lgb_mae}\")\n",
    "print(f\"R² Score: {lgb_r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals\n",
    "residuals_lgb = y_val - y_val_lgb_pred\n",
    "\n",
    "# Residuals Distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(residuals_lgb, kde=True)\n",
    "plt.title('LightGBM Residuals Distribution')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Residuals vs Predicted\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_val_lgb_pred, residuals_lgb, alpha=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.title('LightGBM Residuals vs Predicted')\n",
    "plt.xlabel('Predicted values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- R² Score Comparison ---\")\n",
    "print(f\"Linear Regression R²:      {r2:.4f}\")\n",
    "print(f\"Random Forest R²:          {rf_r2:.4f}\")\n",
    "print(f\"LightGBM Regressor R²:     {lgb_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_lgb = lgb_model.feature_importances_\n",
    "\n",
    "ohe = pipeline.named_steps['preprocessor'].named_transformers_['cat']\n",
    "ohe_feature_names = ohe.get_feature_names_out(categorical_columns)\n",
    "all_feature_names = list(ohe_feature_names) + continuous_columns + ['hr_sin', 'hr_cos', 'weekday_sin', 'weekday_cos']\n",
    "\n",
    "feature_importances_lgb_df = pd.DataFrame({\n",
    "    'Feature': all_feature_names,\n",
    "    'Importance': importances_lgb\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot top 15 important features\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(data=feature_importances_lgb_df.head(15), x='Importance', y='Feature')\n",
    "plt.title('Top 15 Feature Importances - LightGBM')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# overfitting and high variance checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Calculate MSE and R² for the training data\n",
    "y_train_lgb_pred = lgb_model.predict(X_train_transformed)\n",
    "train_mse = mean_squared_error(y_train, y_train_lgb_pred)\n",
    "train_r2 = r2_score(y_train, y_train_lgb_pred)\n",
    "\n",
    "# Calculate MSE and R² for the validation data\n",
    "val_mse = mean_squared_error(y_val, y_val_lgb_pred)\n",
    "val_r2 = r2_score(y_val, y_val_lgb_pred)\n",
    "\n",
    "print(f\"Training MSE: {train_mse}\")\n",
    "print(f\"Training R²: {train_r2}\")\n",
    "print(f\"Validation MSE: {val_mse}\")\n",
    "print(f\"Validation R²: {val_r2}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Signs of overfitting:\n",
    "\n",
    "Training R² is much higher than Validation R².\n",
    "\n",
    "Training MSE is much lower than Validation MSE.\n",
    "Training R² is much higher than Validation R²:\n",
    "\n",
    "Overfitting occurs when the model fits the training data too well, capturing noise or outliers that do not generalize to new, unseen data.\n",
    "\n",
    "If the R² score for the training set is significantly higher than for the validation set, it indicates that the model is fitting the training data too closely and might not generalize well to new data.\n",
    "\n",
    "Training MSE is much lower than Validation MSE:\n",
    "\n",
    "MSE (Mean Squared Error) is a measure of how far off the model’s predictions are from the actual values.\n",
    "\n",
    "If the MSE for the training set is much lower than for the validation set, it suggests that the model is overfitting: it performs very well on the training set (low MSE) but struggles on unseen data (higher MSE on the validation set).\n",
    "\n",
    "Signs of High Variance in Your Model:\n",
    "Let’s focus on the LightGBM model you've been working on:\n",
    "\n",
    "Training R² vs Validation R²:\n",
    "\n",
    "High training R² but low validation R² indicates high variance. If your model has a much higher performance on the training set than on the validation set, it's overfitting and has high variance.\n",
    "\n",
    "Training MSE vs Validation MSE:\n",
    "\n",
    "Similar to the R² comparison, if training MSE is much lower than validation/test MSE, it’s a sign that the model is fitting to noise in the training data, causing poor performance on unseen data (high variance).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Define the model\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Hyperparameter grid (wider ranges for exploration)\n",
    "param_dist = {\n",
    "    'n_estimators': randint(200, 1000),  # Increased number of trees\n",
    "    'max_depth': randint(10, 150),        # Larger range for max depth to capture more complexity\n",
    "    'min_samples_split': randint(2, 20),  # Minimum samples required to split\n",
    "    'min_samples_leaf': randint(1, 20),   # Minimum samples required at leaf\n",
    "    'max_features': ['sqrt', 'log2', None]  # Testing different values for max features\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV with 5-fold cross-validation\n",
    "random_search_rf = RandomizedSearchCV(estimator=rf_model, param_distributions=param_dist, \n",
    "                                      n_iter=100, cv=5, n_jobs=-1, verbose=2, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "random_search_rf.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Best parameters and performance\n",
    "print(\"Best Parameters for Random Forest:\", random_search_rf.best_params_)\n",
    "print(\"Best Cross-Validation Score for Random Forest:\", random_search_rf.best_score_)\n",
    "\n",
    "# Get the best model and evaluate performance on validation set\n",
    "best_rf_model = random_search_rf.best_estimator_\n",
    "y_val_rf_pred = best_rf_model.predict(X_val_transformed)\n",
    "\n",
    "# Evaluate performance on validation set\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "print(f\"Random Forest MSE on Validation Set: {mean_squared_error(y_val, y_val_rf_pred)}\")\n",
    "print(f\"Random Forest R² on Validation Set: {r2_score(y_val, y_val_rf_pred)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-Validation Score vs. Validation R²:\n",
    "\n",
    "The best cross-validation score (0.5155) is relatively close to the R² score on the validation set (0.656), indicating that the model is performing reasonably well.\n",
    "\n",
    "This suggests that the model generalizes well on unseen data, but there might still be room for improvement.\n",
    "\n",
    "MSE on Validation:\n",
    "\n",
    "The MSE of 15933.13 on the validation set is relatively high, which suggests that there’s still a considerable amount of error in the predictions. In real-world cases, you'd want this value to be lower.\n",
    "\n",
    "R² on Validation:\n",
    "\n",
    "The R² value of 0.657 indicates that about 65.7% of the variance in the target variable is explained by the model. This is a decent start, but there is still room to improve.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Evaluate the Random Forest model on the validation set\n",
    "y_val_rf_pred = random_search_rf.best_estimator_.predict(X_val_transformed)\n",
    "\n",
    "# Calculate MSE and R² on the validation set\n",
    "mse_val = mean_squared_error(y_val, y_val_rf_pred)\n",
    "r2_val = r2_score(y_val, y_val_rf_pred)\n",
    "\n",
    "# Print validation performance\n",
    "print(f\"Random Forest MSE on Validation Set: {mse_val}\")\n",
    "print(f\"Random Forest R² on Validation Set: {r2_val}\")\n",
    "\n",
    "# Now, plot the feature importance\n",
    "importances = random_search_rf.best_estimator_.feature_importances_\n",
    "indices = importances.argsort()\n",
    "\n",
    "# Get actual feature names (assuming you're using a ColumnTransformer or OneHotEncoder pipeline)\n",
    "try:\n",
    "    # Try to get feature names from pipeline\n",
    "    feature_names = X_train_transformed.columns\n",
    "except AttributeError:\n",
    "    # If it's not a DataFrame, use the transformer pipeline to get names\n",
    "    try:\n",
    "        feature_names = preprocessor.get_feature_names_out()\n",
    "    except:\n",
    "        feature_names = [f'Feature {i}' for i in range(X_train_transformed.shape[1])]\n",
    "\n",
    "# Plot top N features\n",
    "top_n = min(10, X_train_transformed.shape[1])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Random Forest Feature Importance\")\n",
    "plt.barh(range(top_n), importances[indices[-top_n:]], align=\"center\")\n",
    "plt.yticks(range(top_n), [feature_names[i] for i in indices[-top_n:]])  # Use actual feature names\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
